---
title: "baselineModel"
output: html_document
---
```{r}
library(ggplot2)
library(tidyverse)
library(stringr)
library(GGally)
library(parcoords)
library(tidyr)
library(r2d3)
library(dplyr)
library(readr)
library(gridExtra)
library(plotly)
library(ade4)
library(data.table)
library(caret)
library(devtools) #inorder to use install_version
#most current version of xgboost can't be installed on the Rstudio server, thus
#install the older version: xgboost_0.90.0.2.tar.gz	2019-08-01 21:20	826K
#install_version("xgboost", version = "0.90.0.2", repost = "http://cran.us.r-project.org")
library(xgboost)
library(MLmetrics)#for calculating F1-score
library(ranger) #fast random forest
library(Boruta)
```


```{r}
#install.packages('RJDBC')

## --------------------------------
## Sets Java Home
## --------------------------------
if (Sys.getenv("JAVA_HOME")!="")  Sys.setenv(JAVA_HOME="")


## --------------------------------
## Loads libraries
## --------------------------------
library(rJava)
options(java.parameters = "-Xmx8048m")
library(RJDBC)
library(DBI)


## --------------------------------
## Sets Java Home
## --------------------------------
if (Sys.getenv("JAVA_HOME")!="")
  Sys.setenv(JAVA_HOME="")


## --------------------------------
## Loads libraries
## --------------------------------
library(rJava)
options(java.parameters = "-Xmx8048m")
library(RJDBC)
library(DBI)


## --------------------------------
## General Variables
## --------------------------------
redShiftDriver <- JDBC("com.amazon.redshift.jdbc41.Driver", "RedshiftJDBC41-1.2.8.1005.jar", identifier.quote="`")
# put Redshift jar file in your home directory
resultSet <- data.frame()
username <- "zfan4"               ## Set VPCx / Redshift username - this is your J&J userID, for example mine is stong2  
password <- "19A3#a39F7py"            ## Set VPCx / Redshift password - this is the password you got from an encrypted email
rhealthDBName <- "saf"                ## Set dataset you want to connect to (full list provided here:  https://jnj.sharepoint.com/sites/PHM-GCSP-RND/RWE/Pages/rHEALTH-Database-Connectivity.aspx)


## --------------------------------
## Connection (do not change)
## --------------------------------
connectionString <- paste("jdbc:redshift://rhealth-prod-4.cldcoxyrkflo.us-east-1.redshift.amazonaws.com:5439/", rhealthDBName,"?ssl=true&sslfactory=com.amazon.redshift.ssl.NonValidatingFactory", sep="")
conn <- dbConnect(redShiftDriver, connectionString, username, password)


# dbListTables(conn)

res <- dbSendQuery(conn,'select * from saf.scratch_ctsaf2.np_col_hipfx_ads_v9') ## CTSAF2 is my scratch space
data <- fetch(res,n = -1) 

```

```{r}
global_st <- Sys.time()
df_medicare <- data
dim(df_medicare)
```

Remove target information (247 -184 = 63)
```{r}
columns_having_target_info <- c('mortality_365', 'readm_flag_365', 'er_365days', 're_claim_no',  're_clm_admsn_dt', 're_nch_bene_dschrg_dt','re_prvdr_num', 're_clm_utlztn_day_cnt', 're_clm_pmt_amt', 're_pdx', 're_ppx', 're_adm_source', 're_disc_status', 're_pdx_desc', 're_ppx_desc', 're_clm_admsn_dt_365', 're_nch_bene_dschrg_dt_365', 're_prvdr_num_365', 're_clm_utlztn_day_cnt_365', 're_clm_pmt_amt_365', 're_pdx_365', 're_ppx_365', 're_adm_source_365', 're_claim_no_365', 're_disc_status_365', 're_pdx_desc_365', 're_ppx_desc_365', 're_claim_no_365_up', 'readm_flag_365_up', 're_clm_admsn_dt_365_up', 're_nch_bene_dschrg_dt_365_up', 're_prvdr_num_365_up', 're_clm_utlztn_day_cnt_365_up', 're_clm_pmt_amt_365_up', 're_pdx_365_up', 're_ppx_365_up', 're_adm_source_365_up', 're_disc_status_365_up', 're_pdx_desc_365_up', 're_ppx_desc_365_up', 'er_claim_no_365', 'er_rev_cntr_dt_365', 'er_clm_thru_dt_365', 'er_prncpal_dgns_cd_365', 'er_pdx_desc_365', 'er_hcpcs_cd_365', 'er_hcpcs_desc_365', 'mortality_365_up', 'er_365days_up', 'er_claim_no_365_up', 'er_rev_cntr_dt_365_up', 'er_clm_thru_dt_365_up', 'er_prncpal_dgns_cd_365_up', 'er_pdx_desc_365_up', 'er_hcpcs_cd_365_up', 'er_hcpcs_desc_365_up','er_claim_no', 'er_rev_cntr_dt', 'er_clm_thru_dt','er_prncpal_dgns_cd','er_pdx_desc', 'er_hcpcs_cd', 'er_hcpcs_desc')


df_medicare <- df_medicare[, !(colnames(df_medicare) %in% columns_having_target_info)]
```

Remove collinear features (184 - 152 = 32)
```{r}
columns_having_collinearity <- c('cci_score_1825_days_b','elix_score_1825_days_b',  'fci_score_1825_days_b', 'follow_up_end_dt', 'follow_up_end_dt_365','ppx_desc', 'pdx_desc')

df_medicare <- df_medicare[, !(colnames(df_medicare) %in% columns_having_collinearity)]

#drop hospital name later as needed it for remove na value
add_colinear <- c('prvdr_num','fy','yr_adm','yr_disch','prvdr_state_name','prvdr_state_cd','prvdr_ssa_county_code','prov_vol_per_month','prvdr_home_hha_vol_month','prvdr_home_hha_vol_per','phy_vol_month','prvdr_urspa','prvdr_cbsa','prvdr_cbsa_desc','prvdr_msa','prvdr_msa_desc', "elix_cong_heart_fail_1825_days_b", "elix_periph_vas_dis_1825_days_b", "elix_paralysis_1825_days_b","elix_copd_1825_days_b","elix_aids_1825_days_b", "elix_met_cancer_1825_days_b",     "fci_heart_attack_1825_days_b","fci_obesity_1825_days_b",'nch_bene_dschrg_dt' )


df_medicare <- df_medicare[, !(colnames(df_medicare) %in% add_colinear)]


```

Remove features same entry and unique entry (152 - 138 = 14)
```{r}
columns_no_variance <- c('version_id', 'cont_enroll_flag_1825b_89f', 'valid_date_of_death_1825b_89f', 'hmo_enroll_flag_1825b_89f', 'hmo_enroll_flag_365f','cont_enroll_flag_365f','at_physn_upin', 'op_physn_upin', 'ot_physn_upin')

columns_unique <- c('desy_sort_key', 'claim_no', 'ot_physn_npi','at_physn_npi','op_physn_npi')

df_medicare <- df_medicare[, !(colnames(df_medicare) %in% columns_no_variance)]

df_medicare <- df_medicare[, !(colnames(df_medicare) %in% columns_unique)]
```

Remove missing value (439248 - 429599 = 9649)
```{r}
# remove columns with missing value
# df_medicare <- df_medicare[,df_medicare %>% is.na() %>% colSums() == 0]

# remove rows with missing value
df_medicare[df_medicare == -999] <- NA
df_medicare[df_medicare == 'NA'] <- NA
df_medicare <- df_medicare %>% drop_na(hospital_name,prvdr_teaching_status,ma_pen_percent,prvdr_rday,ami_cabg)
# The following code is to check whether there is still missing value left
# t <- colSums(is.na(df_medicare))
# t[t!=0]
 
#remove hospital_name here as it has collinearity and no need laterr
 df_medicare <- df_medicare[, !(colnames(df_medicare) %in% c("hospital_name"))]
```

Remove columns leaking hospital information
```{r}
# 
# columns_hospital_info <- c('fci_arthritis_1825_days_b', 'fci_osteoporosis_1825_days_b', 'fci_asthma_1825_days_b', 'fci_copd_1825_days_b', 'fci_angina_1825_days_b', 'fci_cong_heart_fail_1825_days_b', 'fci_heart_attack_1825_days_b', 'fci_neur_dis_1825_days_b', 'fci_stroke_1825_days_b', 'fci_diabetes_1825_days_b', 'fci_perif_vasc_dis_1825_days_b', 'fci_upper_gi_1825_days_b', 'fci_depression_1825_days_b', 'fci_anxiety_panic_1825_days_b', 'fci_visual_imp_1825_days_b', 'fci_hear_imp_1825_days_b', 'fci_ddd_1825_days_b', 'fci_obesity_1825_days_b', 
# 'prvdr_state_cd', 'prov_vol_annual', 'prov_vol_per_month', 'prvdr_home_hha_vol', 'prvdr_home_hha_vol_month', 'prvdr_home_hha_vol_per', 'phy_vol_annual', 'phy_vol_month', 'op_phy_cum_exp', 'provider_type', 'hospital_name', 'prvdr_ssa_county_code', 'prvdr_urgeo', 'prvdr_urspa', 'prvdr_wi', 'prvdr_cola', 'prvdr_resident_to_bed_ratio', 'prvdr_rday', 'prvdr_beds', 'prvdr_dshpct', 'prvdr_dshopg', 'prvdr_dshcpg', 'prvdr_state_name', 'prvdr_state_ab', 'prvdr_div_code', 'prvdr_division', 'prvdr_region_cd', 'prvdr_region','county', 'bpci_lejr', 'cjr', 'bpci_ami_cabg', 'ami_cabg', 'prvdr_cbsa', 'prvdr_cbsa_desc', 'prvdr_msa', 'prvdr_msa_desc', 'prvdr_teaching_status', 'low_income_subsidy', 'prior_inp_only_los', 'prior_irf_los', 'prior_ltcf_los', 'prior_snf_los', 'prior_hha_visits', 'prior_out_visits', 'total_prior_los', 'poverty_per', 'grp_1', 'grp_2', 'grp_3', 'grp_4', 'grp_5', 'grp_6', 'grp_7', 'grp_8', 'grp_9', 'grp_10', 'unemp_percentage', 'no_of_snfs', 'ma_pen_percent', 'snf_per_capita')
# 
# 
# df_medicare <- df_medicare[, !(colnames(df_medicare) %in% columns_hospital_info)]
# 

```


```{r}
###df_medicare_state <- df_medicare %>% group_by(prvdr_state_name) %>% summarise(number = length(prvdr_state_name), sample = as.integer(length(prvdr_state_name)/10) )
## for histogram (entire dataset)

# ----------------- 
#Population Sampling
# -----------------
set.seed(100) #for reproduce purpose
population_sample <- df_medicare %>% group_by(prvdr_state_ab) %>% sample_n(as.integer(ceiling(length(prvdr_state_ab)/20)))
```

```{r}
# -------------------
# feature engineering
# -------------------


# Add time related features for later use
population_sample$clm_admsn_dt = as.Date(population_sample$clm_admsn_dt)
population_sample$month = months(population_sample$clm_admsn_dt,abbreviate = TRUE)
population_sample$week_of_dates = weekdays(population_sample$clm_admsn_dt,abbreviate = TRUE)
#change difftime object to numeric
population_sample$days_before_admsn = as.numeric(population_sample$clm_admsn_dt-as.Date('2016-01-01'), units="days")
#no need the days_before_dscharg as we already have length of stay and is removed from data in the collinearity section
#remove date column as we have convert it to number
drops = c('clm_admsn_dt')
population_sample = population_sample[,!(colnames(population_sample) %in% drops)]

#for later target encoder use
population_sample[sapply(population_sample, is.character)] <- lapply(population_sample[sapply(population_sample, is.character)], as.factor)
```


```{r}
# ----------------- 
# train validation test split
# -----------------

# we focus on readmission now. so we remove mortality and er_90days 
drops<- c('mortality_90', 'er_90days')
population_sample <- population_sample[,!(colnames(population_sample) %in% drops)]

# hold out test set
population_sample = population_sample[order(population_sample$days_before_admsn),]
size = nrow(population_sample)
#create train for training model with cv method
train_size = as.integer(round(size*0.7,digits=0))
#crerate val for comparing models
val_size = as.integer(round(size*0.15,digits=0))
#create hold out set only use it once when determine the final model
test_size = as.integer(size - train_size - val_size)

train = population_sample[c(1:train_size), ]
val = population_sample[c((train_size+1): (train_size + val_size)), ]
test = population_sample[c((train_size + val_size +1):size), ]

```


```{r}
#target encoding for categorical features

cat_name = names(Filter(is.factor, train))
cat_index = match(cat_name, names(train))
targetencode = function(indices, df){
  result = c()
  for (i in indices){
    lookup = train %>% group_by_at(i) %>% summarise(mean(readm_flag))
    result = c(result, lookup)
  }
  return(result)
}

targetencode_transform = function(indices, result, df, dropna = TRUE){
  j = 1
  for (i in indices){
    lookup = data.frame(result[j], result[j+1])
    cat_colname = colnames(df)[i]
    df_targetment = left_join(df[cat_colname], lookup, by = cat_colname)
    df[i] = df_targetment[, 2]
    j = j +2
  }
  if (dropna & sum(is.na(df))){
    print("There is missing categorical class")
    t <- colSums(is.na(df))
    print(t[t!=0])
    df = df %>% drop_na()
  }
  return(df)
  }
tgen = targetencode(cat_index, train)
train_tgen = targetencode_transform(cat_index, tgen, train)
val_tgen = targetencode_transform(cat_index, tgen, val)
test_tgen = targetencode_transform(cat_index, tgen, test)

```


Further Testing collinearity with findLinearCombos
```{r}
#since findlinearcombos only works for numeric values, thus apply here after target encoding
comboInfo <- findLinearCombos(train_tgen)
comboInfo
```

Data preprocessing to put into "train" function
```{r}
#Due to computing power, start with 200 observations first, and 100 for validation set, 100 for test set
sm_train_size = 1000
sm_val_size = 500
sm_test_size = 500

train = train_tgen[c(1:sm_train_size),]
val = val_tgen[c(1:sm_val_size),]
test = test_tgen[c(1:sm_test_size),]

#R caret cv doesn't do stratified, meaning we might have missing class in the sample
#Thus fix it by creating index inadvance
#Reference Code: https://stackoverflow.com/questions/35907477/caret-package-stratified-cross-validation-in-train-function
folds <- 5
cvIndex <- createFolds(train$readm_flag, folds, returnTrain = T)

#define variable name for 0 and 1 due to classProbs = TRUE and prSummary in traincontrol
train$readm_flag =as.factor(train$readm_flag) 
levels(train$readm_flag)= c("Class_0", "Class_1")
train$readm_flag <- factor(train$readm_flag, levels=rev(levels(train$readm_flag)))

#create y_train, y_val, and y_test as they are all the same for all type of feature combinations
y_train = train$readm_flag
y_val = val$readm_flag
y_test = test$readm_flag
```


Base Model data set

```{r}

# dataset for base model (remove time related features)
time_feature = c('month','week_of_dates','days_before_admsn')
train_base = train[,!(colnames(train) %in% time_feature)]
val_base = val[,!(colnames(val) %in% time_feature)]
test_base = test[,!(colnames(test) %in% time_feature)]

#Create xgb.DMatrix specially for XGboost model, run faster
#Need numeric to turn into xgb.Dmatrix
target = c("readm_")
X_train_base = train_base %>% select(-readm_flag)
X_train_base_xgb = xgb.DMatrix(as.matrix(X_train_base))
X_val_base = val_base %>% select(-readm_flag)
X_val_base_xgb = xgb.DMatrix(as.matrix(X_val_base))
X_test_base = test_base %>% select(-readm_flag)
X_test_base_xgb = xgb.DMatrix(as.matrix(X_test_base))

train_combined_base= train_base

```


Boruta with no time data set

```{r}
boruta_start_time <- Sys.time()
sigtest <- Boruta(readm_flag~. , data = train_base, holdHistory=FALSE, doTrace=2)   # reduce memory footprint: https://cran.r-project.org/web/packages/Boruta/vignettes/inahurry.pdf
boruta_end_time <- Sys.time()
print(boruta_end_time - boruta_start_time)
boruta_features = getSelectedAttributes(sigtest)
X_train_bor = train[, boruta_features]
X_train_bor_xgb = xgb.DMatrix(as.matrix(X_train_bor))
X_val_bor = val[, boruta_features]
X_val_bor_xgb = xgb.DMatrix(as.matrix(X_val_bor))
print(boruta_features)

train_combined_bor = train[, c(boruta_features, "readm_flag")]
```

Boruta with time data set

```{r}

boruta_time_features = c(boruta_features, time_feature)
X_train_bor_time = train[, boruta_time_features]
X_train_bor_time_xgb = xgb.DMatrix(as.matrix(X_train_bor_time))
X_val_bor_time = val[, boruta_time_features]
X_val_bor_time_xgb = xgb.DMatrix(as.matrix(X_val_bor_time))
train_combined_bor_time = train[, c(boruta_time_features, "readm_flag")]

```
Base with time data set

```{r}
X_train_time = train %>% select(-readm_flag)
X_train_time_xgb = xgb.DMatrix(as.matrix(X_train_time))
X_val_time = val %>% select(-readm_flag)
X_val_time_xgb = xgb.DMatrix(as.matrix(X_val_time))
train_combined_time = train
```


```{r}
#F1 Score note

#Note if TP + FP ==0 (when we all predict 0), precision is undefined.
#Precision = TP / (TP+FP)
#Recall = TP/(TP + FN)

#if TP + FN == 0 (when sample only have 0), recall is undefined
#F1 = 2 * Precision * Recall/ (Precision + Recall) -> undefined if TP = 0


# ConfusionMatrix
#         ACTUAL
#        1     0
# P    +----+----+
# R  1 | TP | FP |
# E    +----+----+
# D  0 | FN | TN |
#      +----+----+

# since if we predict all as Class 0 or as Class 1, F1_score will return error, plus we don't want a model that predict all as 0 or 1, thus set its f1 score = -1 indicate bad model
unbias_f1 = function(pred, yval){
  f1 = -1
  if (sum(pred)!= 0 & sum(pred)!= length(pred)){
    f1 = F1_Score( y_pred = pred, y_true = yval, positive = 1)
  }
  return(f1)
}

```


XGboost-Based

```{r}
#Reference: https://datascienceplus.com/extreme-gradient-boosting-with-r/
#Specify cross-validation method and number of folds. Also enable parallel computation
xgb_base_start_time <- Sys.time()
xgb_trcontrol = trainControl(
  index = cvIndex,
  method = "cv",
  number = folds,  
  allowParallel = TRUE,
  verboseIter = FALSE,
  classProbs = TRUE,
  #Need to install "MLmetrics" packages
  summaryFunction = prSummary,
  returnData = FALSE
)

xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
                       max_depth = c(10, 15, 20, 25),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## The values below are default values in the sklearn-api. 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
                      )
set.seed(0) 
xgb_base_fit = train(
  X_train_base_xgb, y_train,  
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "F"
)
xgb_base_end_time <- Sys.time()
print(xgb_base_end_time - xgb_base_start_time)
```

```{r}
#Summary on the model
xgb_base_fit$results
#best tune result
xgb_base_fit$bestTune
#obtain the best f1 score from the result
xgb_base_trainval_sc = max(xgb_base_fit$results$F)
print(paste("XGboost base model best train_val score: ", as.character(xgb_base_trainval_sc)))

```



```{r}
#Prediction by class to get F1_Score
pre <- predict(xgb_base_fit, newdata = X_val_base, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
xgb_base_val_f1score =unbias_f1(pred, y_val)

print(paste("XGboost base model val F1 score: ", as.character(xgb_base_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
xgb_prediction <- predict(xgb_base_fit, newdata = X_val_base, type = "prob")
xgb_y_pred = xgb_prediction$Class_1
xgb_y_true = y_val


# get metrix results
xgb_base_LogLoss = LogLoss(xgb_y_pred, xgb_y_true)
xgb_base_AUC = AUC(xgb_y_pred, xgb_y_true)
xgb_base_acc = Accuracy(round(xgb_y_pred), xgb_y_true)
print(paste("XGboost base model logloss on val: ", as.character(xgb_base_LogLoss)))
print(paste("XGboost base model AUC on val: ", as.character(xgb_base_AUC)))
print(paste("XGboost base model acc on val: ", as.character(xgb_base_acc)))
```

XGboost with Boruta
```{r}
start_time <- Sys.time()
set.seed(0) 
xgb_bor_fit = train(
  X_train_bor_xgb, y_train,  
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "F"
)
end_time <- Sys.time()
print(end_time - start_time)
```
```{r}
#Summary on the model
xgb_bor_fit$results
#best tune result
xgb_bor_fit$bestTune
#obtain the best f1 score from the result
xgb_bor_trainval_sc = max(xgb_bor_fit$results$F)
print(paste("XGboost boruta model best train_val score: ", as.character(xgb_bor_trainval_sc)))

```



```{r}
#Prediction by class to get F1_Score
pre <- predict(xgb_bor_fit, newdata = X_val_bor, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
xgb_bor_val_f1score = unbias_f1(pred, y_val)

print(paste("XGboost Boruta model val F1 score: ", as.character(xgb_bor_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
xgb_prediction <- predict(xgb_bor_fit, newdata = X_val_bor, type = "prob")
xgb_y_pred = xgb_prediction$Class_1
xgb_y_true = y_val


# get metrix results
xgb_bor_LogLoss = LogLoss(xgb_y_pred, xgb_y_true)
xgb_bor_AUC = AUC(xgb_y_pred, xgb_y_true)
xgb_bor_acc = Accuracy(round(xgb_y_pred), xgb_y_true)
print(paste("XGboost Boruta model logloss on val: ", as.character(xgb_bor_LogLoss)))
print(paste("XGboost Boruta model AUC on val: ", as.character(xgb_bor_AUC)))
print(paste("XGboost Boruta model acc on val: ", as.character(xgb_bor_acc)))
```

XGBoost with boruta and time relate features
```{r}
start_time <- Sys.time()
set.seed(0) 
xgb_bor_time_fit = train(
  X_train_bor_time_xgb, y_train,  
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "F"
)
end_time <- Sys.time()
print(end_time - start_time)
```
```{r}
#Summary on the model
xgb_bor_time_fit$results
#best tune result
xgb_bor_time_fit$bestTune
#obtain the best f1 score from the result
xgb_bor_time_trainval_sc = max(xgb_bor_time_fit$results$F)
print(paste("XGboost boruta model with time features best train_val score: ", as.character(xgb_bor_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(xgb_bor_time_fit, newdata = X_val_bor_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
xgb_bor_time_val_f1score = unbias_f1(pred, y_val)

print(paste("XGboost Boruta model with time features val F1 score: ", as.character(xgb_bor_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
xgb_prediction <- predict(xgb_bor_time_fit, newdata = X_val_bor_time, type = "prob")
xgb_y_pred = xgb_prediction$Class_1
xgb_y_true = y_val


# get metrix results
xgb_bor_time_LogLoss = LogLoss(xgb_y_pred, xgb_y_true)
xgb_bor_time_AUC = AUC(xgb_y_pred, xgb_y_true)
xgb_bor_time_acc = Accuracy(round(xgb_y_pred), xgb_y_true)
print(paste("XGboost Boruta model with time features logloss on val: ", as.character(xgb_bor_time_LogLoss)))
print(paste("XGboost Boruta model with time features AUC on val: ", as.character(xgb_bor_time_AUC)))
print(paste("XGboost Boruta model with time features acc on val: ", as.character(xgb_bor_time_acc)))
```

XGBoost with time relate features no boruta

```{r}
start_time <- Sys.time()
set.seed(0) 
xgb_time_fit = train(
  X_train_time_xgb, y_train,  
  trControl = xgb_trcontrol,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "F"
)
end_time <- Sys.time()
print(end_time - start_time)
```
```{r}
#Summary on the model
xgb_time_fit$results
#best tune result
xgb_time_fit$bestTune
#obtain the best f1 score from the result
xgb_time_trainval_sc = max(xgb_time_fit$results$F)
print(paste("XGboost base + time features model best train_val score: ", as.character(xgb_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(xgb_time_fit, newdata = X_val_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
xgb_time_val_f1score = unbias_f1(pred, y_val)

print(paste("XGboost Base model with time features val F1 score: ", as.character(xgb_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
xgb_prediction <- predict(xgb_time_fit, newdata = X_val_time, type = "prob")
xgb_y_pred = xgb_prediction$Class_1
xgb_y_true = y_val


# get metrix results
xgb_time_LogLoss = LogLoss(xgb_y_pred, xgb_y_true)
xgb_time_AUC = AUC(xgb_y_pred, xgb_y_true)
xgb_time_acc = Accuracy(round(xgb_y_pred), xgb_y_true)
print(paste("XGboost based model with time features logloss on val: ", as.character(xgb_time_LogLoss)))
print(paste("XGboost based model with time features AUC on val: ", as.character(xgb_time_AUC)))
print(paste("XGboost based model with time features acc on val: ", as.character(xgb_time_acc)))

```

```{r}
#Summary on Xgboost Model
ft_sel_methods = c("based", "boruta", "boruta_time", "base_time")
xgb_trvl = c(xgb_base_trainval_sc, xgb_bor_trainval_sc, xgb_bor_time_trainval_sc, xgb_time_trainval_sc)
xgb_val_f1score = c(xgb_base_val_f1score, xgb_bor_val_f1score, xgb_bor_time_val_f1score, xgb_time_val_f1score)
xgb_LogLoss = c(xgb_base_LogLoss,xgb_bor_LogLoss,xgb_bor_time_LogLoss,xgb_time_LogLoss)
xgb_AUC = c(xgb_base_AUC,xgb_bor_AUC,xgb_bor_time_AUC,xgb_time_AUC)

print(ft_sel_methods)
print("train_val score")
print(xgb_trvl)
print("Val F1 Score")
print(xgb_val_f1score)
print("LogLoss")
print(xgb_LogLoss)
print("AUC")
print(xgb_AUC)

print("Summary on XGboost")
print(paste("XGboost with highest train_val score:", ft_sel_methods[which.max(xgb_trvl)], as.character(max(xgb_trvl)), sep = " "))
print(paste("XGboost with highest val f1 score:", ft_sel_methods[which.max(xgb_val_f1score)], as.character(max(xgb_val_f1score)), sep = " "))

print(paste("XGboost with lowest logloss:", ft_sel_methods[which.min(xgb_LogLoss)], as.character(min(xgb_LogLoss)), sep = " "))

```

Random Forest

Base-Model

```{r}
start_time <- Sys.time()
rf_trcontrol = trainControl(index = cvIndex,
                              method = "cv",
                              number = folds,
                              summaryFunction = prSummary,
                              classProbs = TRUE)
set.seed(0)
rf_base_fit <- train(
  form = readm_flag ~., 
  data =train_combined_base,
  trControl= rf_trcontrol,
  method = 'ranger',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rf_base_fit$results
#best tune result
rf_base_fit$bestTune
#obtain the best f1 score from the result
rf_base_trainval_sc = max(rf_base_fit$results$F, na.rm=TRUE)
print(paste("Random Forest base model best train_val score: ", as.character(rf_base_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rf_base_fit, newdata = X_val_base, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rf_base_val_f1score = unbias_f1(pred, y_val)

print(paste("Random forest base model val F1 score: ", as.character(rf_base_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rf_prediction <- predict(rf_base_fit, newdata = X_val_base, type = "prob")
rf_y_pred = rf_prediction$Class_1
rf_y_true = y_val


# get metrix results
rf_base_LogLoss = LogLoss(rf_y_pred, rf_y_true)
rf_base_AUC = AUC(rf_y_pred, rf_y_true)
rf_base_acc = Accuracy(round(rf_y_pred), rf_y_true)
print(paste("Random forest base model logloss on val: ", as.character(rf_base_LogLoss)))
print(paste("Random forest base model AUC on val: ", as.character(rf_base_AUC)))
print(paste("Random forest base model acc on val: ", as.character(rf_base_acc)))
```
Random Forest

Boruta-Model

```{r}
start_time <- Sys.time()
set.seed(0)
rf_bor_fit <- train(
  form = readm_flag ~., 
  data =train_combined_bor,
  trControl= rf_trcontrol,
  method = 'ranger',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rf_bor_fit$results
#best tune result
rf_bor_fit$bestTune
#obtain the best f1 score from the result
rf_bor_trainval_sc = max(rf_bor_fit$results$F, na.rm=TRUE)
print(paste("Random Forest Burota model best train_val score: ", as.character(rf_bor_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rf_bor_fit, newdata = X_val_bor, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rf_bor_val_f1score = unbias_f1(pred, y_val)

print(paste("Random forest Boruta model val F1 score: ", as.character(rf_bor_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rf_prediction <- predict(rf_bor_fit, newdata = X_val_bor, type = "prob")
rf_y_pred = rf_prediction$Class_1
rf_y_true = y_val


# get metrix results
rf_bor_LogLoss = LogLoss(rf_y_pred, rf_y_true)
rf_bor_AUC = AUC(rf_y_pred, rf_y_true)
rf_bor_acc = Accuracy(round(rf_y_pred), rf_y_true)
print(paste("Random forest Boruta model logloss on val: ", as.character(rf_bor_LogLoss)))
print(paste("Random forest Boruta model AUC on val: ", as.character(rf_bor_AUC)))
print(paste("Random forest Boruta model acc on val: ", as.character(rf_bor_acc)))
```


Random Forest

Boruta-time Model

```{r}
start_time <- Sys.time()
set.seed(0)
rf_bor_time_fit <- train(
  form = readm_flag ~., 
  data =train_combined_bor_time,
  trControl= rf_trcontrol,
  method = 'ranger',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)

```

```{r}
#Summary on the model
rf_bor_time_fit$results
#best tune result
rf_bor_time_fit$bestTune
#obtain the best f1 score from the result
rf_bor_time_trainval_sc = max(rf_bor_time_fit$results$F, na.rm=TRUE)
print(paste("Random Forest Burota + time features model best train_val score: ", as.character(rf_bor_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rf_bor_time_fit, newdata = X_val_bor_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rf_bor_time_val_f1score = unbias_f1(pred, y_val)

print(paste("Random forest Boruta + time features model val F1 score: ", as.character(rf_bor_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rf_prediction <- predict(rf_bor_time_fit, newdata = X_val_bor_time, type = "prob")
rf_y_pred = rf_prediction$Class_1
rf_y_true = y_val


# get metrix results
rf_bor_time_LogLoss = LogLoss(rf_y_pred, rf_y_true)
rf_bor_time_AUC = AUC(rf_y_pred, rf_y_true)
rf_bor_time_acc = Accuracy(round(rf_y_pred), rf_y_true)
print(paste("Random forest Boruta + time features model logloss on val: ", as.character(rf_bor_time_LogLoss)))
print(paste("Random forest Boruta + time features model AUC on val: ", as.character(rf_bor_time_AUC)))
print(paste("Random forest Boruta + time features model acc on val: ", as.character(rf_bor_time_acc)))
```

Random Forest

Based-time Model

```{r}
start_time <- Sys.time()
set.seed(0)
rf_time_fit <- train(
  form = readm_flag ~., 
  data =train_combined_time,
  trControl= rf_trcontrol,
  method = 'ranger',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rf_time_fit$results
#best tune result
rf_time_fit$bestTune
#obtain the best f1 score from the result
rf_time_trainval_sc = max(rf_time_fit$results$F, na.rm=TRUE)
print(paste("Random Forest Base + time features model best train_val score: ", as.character(rf_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rf_time_fit, newdata = X_val_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rf_time_val_f1score = unbias_f1(pred, y_val)

print(paste("Random forest Base + time features model val F1 score: ", as.character(rf_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rf_prediction <- predict(rf_time_fit, newdata = X_val_time, type = "prob")
rf_y_pred = rf_prediction$Class_1
rf_y_true = y_val


# get metrix results
rf_time_LogLoss = LogLoss(rf_y_pred, rf_y_true)
rf_time_AUC = AUC(rf_y_pred, rf_y_true)
rf_time_acc = Accuracy(round(rf_y_pred), rf_y_true)
print(paste("Random forest Boruta + time features model logloss on val: ", as.character(rf_time_LogLoss)))
print(paste("Random forest Boruta + time features model AUC on val: ", as.character(rf_time_AUC)))
print(paste("Random forest Boruta + time features model acc on val: ", as.character(rf_time_acc)))
```

```{r}
#Summary on Random forest
ft_sel_methods = c("based", "boruta", "boruta_time", "base_time")
rf_trvl = c(rf_base_trainval_sc, rf_bor_trainval_sc, rf_bor_time_trainval_sc, rf_time_trainval_sc)
rf_val_f1score = c(rf_base_val_f1score, rf_bor_val_f1score, rf_bor_time_val_f1score, rf_time_val_f1score)
rf_LogLoss = c(rf_base_LogLoss,rf_bor_LogLoss,rf_bor_time_LogLoss,rf_time_LogLoss)
rf_AUC = c(rf_base_AUC,rf_bor_AUC,rf_bor_time_AUC,rf_time_AUC)

print(ft_sel_methods)
print("train_val score")
print(rf_trvl)
print("Val F1 Score")
print(rf_val_f1score)
print("LogLoss")
print(rf_LogLoss)
print("AUC")
print(rf_AUC)

print("Summary on Random forest")
print(paste("Random forest with highest train_val score:", ft_sel_methods[which.max(rf_trvl)], as.character(max(rf_trvl)), sep = " "))
print(paste("Random forest with highest val f1 score:", ft_sel_methods[which.max(rf_val_f1score)], as.character(max(rf_val_f1score)), sep = " "))

print(paste("Random forest with lowest logloss:", ft_sel_methods[which.min(rf_LogLoss)], as.character(min(rf_LogLoss)), sep = " "))
```


Reglogistic Regression

Base-Model


```{r}
rlg_trcontrol = trainControl(index = cvIndex,
                              method = "cv",
                              number = folds,
                              summaryFunction = prSummary,
                              classProbs = TRUE)
set.seed(0)
start_time  <- Sys.time()
rlg_base_fit <- train(
  form = readm_flag ~., 
  data =train_combined_base,
  trControl= rlg_trcontrol,
  method = 'regLogistic',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rlg_base_fit$results
#best tune result
rlg_base_fit$bestTune
#obtain the best f1 score from the result
rlg_base_trainval_sc = max(rlg_base_fit$results$F, na.rm=TRUE)
print(paste("regLogistic base model best train_val score: ", as.character(rlg_base_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rlg_base_fit, newdata = X_val_base, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rlg_base_val_f1score = unbias_f1(pred, y_val)

print(paste("regLogistic base model val F1 score: ", as.character(rlg_base_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rlg_prediction <- predict(rlg_base_fit, newdata = X_val_base, type = "prob")
rlg_y_pred = rf_prediction$Class_1
rlg_y_true = y_val


# get metrix results
rlg_base_LogLoss = LogLoss(rf_y_pred, rf_y_true)
rlg_base_AUC = AUC(rf_y_pred, rf_y_true)
rlg_base_acc = Accuracy(round(rf_y_pred), rf_y_true)
print(paste("regLogistic base model logloss on val: ", as.character(rlg_base_LogLoss)))
print(paste("regLogistic base model AUC on val: ", as.character(rlg_base_AUC)))
print(paste("regLogistic base model acc on val: ", as.character(rlg_base_acc)))
```

regLogistic
Boruta-Model

```{r}
set.seed(0)
start_time <- Sys.time()
rlg_bor_fit <- train(
  form = readm_flag ~., 
  data =train_combined_bor,
  trControl= rf_trcontrol,
  method = 'regLogistic',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rlg_bor_fit$results
#best tune result
rlg_bor_fit$bestTune
#obtain the best f1 score from the result
rlg_bor_trainval_sc = max(rlg_bor_fit$results$F, na.rm=TRUE)
print(paste("regLogistic Burota model best train_val score: ", as.character(rlg_bor_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rlg_bor_fit, newdata = X_val_bor, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rlg_bor_val_f1score = unbias_f1(pred, y_val)

print(paste("regLogistic Boruta model val F1 score: ", as.character(rlg_bor_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rlg_prediction <- predict(rlg_bor_fit, newdata = X_val_bor, type = "prob")
rlg_y_pred = rlg_prediction$Class_1
rlg_y_true = y_val


# get metrix results
rlg_bor_LogLoss = LogLoss(rlg_y_pred, rlg_y_true)
rlg_bor_AUC = AUC(rlg_y_pred, rlg_y_true)
rlg_bor_acc = Accuracy(round(rlg_y_pred), rlg_y_true)
print(paste("regLogistic Boruta model logloss on val: ", as.character(rlg_bor_LogLoss)))
print(paste("regLogistic Boruta model AUC on val: ", as.character(rlg_bor_AUC)))
print(paste("regLogistic Boruta model acc on val: ", as.character(rlg_bor_acc)))
```


regLogistic

Boruta-time Model

```{r}
start_time <- Sys.time()
rlg_bor_time_fit <- train(
  form = readm_flag ~., 
  data =train_combined_bor_time,
  trControl= rf_trcontrol,
  method = 'regLogistic',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rlg_bor_time_fit$results
#best tune result
rlg_bor_time_fit$bestTune
#obtain the best f1 score from the result
rlg_bor_time_trainval_sc = max(rlg_bor_time_fit$results$F, na.rm=TRUE)
print(paste("Random Forest Burota + time features model best train_val score: ", as.character(rlg_bor_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rlg_bor_time_fit, newdata = X_val_bor_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rlg_bor_time_val_f1score = unbias_f1(pred, y_val)

print(paste("regLogistic Boruta + time features model val F1 score: ", as.character(rlg_bor_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rlg_prediction <- predict(rlg_bor_time_fit, newdata = X_val_bor_time, type = "prob")
rlg_y_pred = rlg_prediction$Class_1
rlg_y_true = y_val


# get metrix results
rlg_bor_time_LogLoss = LogLoss(rlg_y_pred, rlg_y_true)
rlg_bor_time_AUC = AUC(rlg_y_pred, rlg_y_true)
rlg_bor_time_acc = Accuracy(round(rlg_y_pred), rlg_y_true)
print(paste("Random forest Boruta + time features model logloss on val: ", as.character(rlg_bor_time_LogLoss)))
print(paste("Random forest Boruta + time features model AUC on val: ", as.character(rlg_bor_time_AUC)))
print(paste("Random forest Boruta + time features model acc on val: ", as.character(rlg_bor_time_acc)))
```

regLogistic

Based-time Model

```{r}
set.seed(0)
start_time <- Sys.time()
rlg_time_fit <- train(
  form = readm_flag ~., 
  data =train_combined_time,
  trControl= rf_trcontrol,
  method = 'regLogistic',
  metric = "F")
end_time <- Sys.time()
print(end_time - start_time)
```

```{r}
#Summary on the model
rlg_time_fit$results
#best tune result
rlg_time_fit$bestTune
#obtain the best f1 score from the result
rlg_time_trainval_sc = max(rf_time_fit$results$F, na.rm=TRUE)
print(paste("regLogistic Base + time features model best train_val score: ", as.character(rlg_time_trainval_sc)))

```

```{r}
#Prediction by class to get F1_Score
pre <- predict(rlg_time_fit, newdata = X_val_time, type = "raw")
pred =ifelse(pre  == "Class_0",0,1)
rlg_time_val_f1score = unbias_f1(pred, y_val)

print(paste("regLogistic Base + time features model val F1 score: ", as.character(rlg_time_val_f1score)))
```

```{r}
#Prediction by prob to get logloss
rlg_prediction <- predict(rlg_time_fit, newdata = X_val_time, type = "prob")
rlg_y_pred = rlg_prediction$Class_1
rlg_y_true = y_val


# get metrix results
rlg_time_LogLoss = LogLoss(rlg_y_pred, rlg_y_true)
rlg_time_AUC = AUC(rlg_y_pred, rlg_y_true)
rlg_time_acc = Accuracy(round(rlg_y_pred), rlg_y_true)
print(paste("Random forest Boruta + time features model logloss on val: ", as.character(rlg_time_LogLoss)))
print(paste("Random forest Boruta + time features model AUC on val: ", as.character(rlg_time_AUC)))
print(paste("Random forest Boruta + time features model acc on val: ", as.character(rlg_time_acc)))
```

```{r}
#Summary on regLogistic
ft_sel_methods = c("based", "boruta", "boruta_time", "base_time")
rlg_trvl = c(rlg_base_trainval_sc, rlg_bor_trainval_sc, rlg_bor_time_trainval_sc, rlg_time_trainval_sc)
rlg_val_f1score = c(rlg_base_val_f1score, rlg_bor_val_f1score, rlg_bor_time_val_f1score, rlg_time_val_f1score)
rlg_LogLoss = c(rlg_base_LogLoss,rlg_bor_LogLoss,rlg_bor_time_LogLoss,rlg_time_LogLoss)
rlg_AUC = c(rlg_base_AUC,rlg_bor_AUC,rlg_bor_time_AUC,rlg_time_AUC)

print(ft_sel_methods)
print("train_val score")
print(rlg_trvl)
print("Val F1 Score")
print(rlg_val_f1score)
print("LogLoss")
print(rlg_LogLoss)
print("AUC")
print(rlg_AUC)

print("Summary on regLogistic")
print(paste("regLogistic with highest train_val score:", ft_sel_methods[which.max(rlg_trvl)], as.character(max(rlg_trvl)), sep = " "))
print(paste("regLogistic with highest val f1 score:", ft_sel_methods[which.max(rlg_val_f1score)], as.character(max(rlg_val_f1score)), sep = " "))

print(paste("regLogistic with lowest logloss:", ft_sel_methods[which.min(rlg_LogLoss)], as.character(min(rlg_LogLoss)), sep = " "))
```

```{r}
#Final Summary
ft_sel_methods = c("xgbbased", "xgbboruta", "xgbboruta_time", "xgbbase_time", "rfbased", "rfboruta", "rfboruta_time", "rfbase_time", "rlgbased", "rlgboruta", "rlgboruta_time", "rlgbase_time")

print(ft_sel_methods)
print("train_val score")
all_trvl = c(xgb_trvl, rf_trvl, rlg_trvl)
print(all_trvl)
print("Val F1 Score")
all_val_f1score = c(xgb_val_f1score, rf_val_f1score, xgb_val_f1score)
print(all_val_f1score)
print("LogLoss")
all_LogLoss = c(xgb_LogLoss,rf_LogLoss, rlg_LogLoss)
print(all_LogLoss)
print("AUC")
print(c(xgb_AUC,rf_AUC, rlg_AUC))


print("Summary on all")
print(paste("Highest train_val score:", ft_sel_methods[which.max(all_trvl)], as.character(max(all_trvl)), sep = " "))
print(paste("Highest val f1 score:", ft_sel_methods[which.max(all_val_f1score)], as.character(max(all_val_f1score)), sep = " "))

print(paste("Lowest logloss:", ft_sel_methods[which.min(all_LogLoss)], as.character(min(all_LogLoss)), sep = " "))
global_ed <- Sys.time()
print(global_ed - global_st)
```


