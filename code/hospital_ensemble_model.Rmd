---
title: "baselineModel"
output: html_document
---
```{r}
library(ggplot2)
library(tidyverse)
library(stringr)
library(GGally)
library(parcoords)
library(tidyr)
library(r2d3)
library(dplyr)
library(readr)
library(gridExtra)
library(plotly)
library(ade4)
library(data.table)
library(caret)
library(devtools) #inorder to use install_version
#most current version of xgboost can't be installed on the Rstudio server, thus
#install the older version: xgboost_0.90.0.2.tar.gz	2019-08-01 21:20	826K
#install_version("xgboost", version = "0.90.0.2", repost = "http://cran.us.r-project.org")
library(xgboost)
library(MLmetrics)#for calculating F1-score
library(ranger) #fast random forest
library(Boruta)
library(FSinR)
#rm(list=setdiff(ls(), "data")) command to clean the environment list
```


```{r}
#install.packages('RJDBC')

## --------------------------------
## Sets Java Home
## --------------------------------
if (Sys.getenv("JAVA_HOME")!="")  Sys.setenv(JAVA_HOME="")


## --------------------------------
## Loads libraries
## --------------------------------
library(rJava)
options(java.parameters = "-Xmx8048m")
library(RJDBC)
library(DBI)


## --------------------------------
## Sets Java Home
## --------------------------------
if (Sys.getenv("JAVA_HOME")!="")
  Sys.setenv(JAVA_HOME="")


## --------------------------------
## Loads libraries
## --------------------------------
library(rJava)
options(java.parameters = "-Xmx8048m")
library(RJDBC)
library(DBI)


## --------------------------------
## General Variables
## --------------------------------
redShiftDriver <- JDBC("com.amazon.redshift.jdbc41.Driver", "RedshiftJDBC41-1.2.8.1005.jar", identifier.quote="`")
# put Redshift jar file in your home directory
resultSet <- data.frame()
username <- "zfan4"               ## Set VPCx / Redshift username - this is your J&J userID, for example mine is stong2  
password <- "19A3#a39F7py"            ## Set VPCx / Redshift password - this is the password you got from an encrypted email
rhealthDBName <- "saf"                ## Set dataset you want to connect to (full list provided here:  https://jnj.sharepoint.com/sites/PHM-GCSP-RND/RWE/Pages/rHEALTH-Database-Connectivity.aspx)


## --------------------------------
## Connection (do not change)
## --------------------------------
connectionString <- paste("jdbc:redshift://rhealth-prod-4.cldcoxyrkflo.us-east-1.redshift.amazonaws.com:5439/", rhealthDBName,"?ssl=true&sslfactory=com.amazon.redshift.ssl.NonValidatingFactory", sep="")
conn <- dbConnect(redShiftDriver, connectionString, username, password)


# dbListTables(conn)

res <- dbSendQuery(conn,'select * from saf.scratch_ctsaf2.np_col_hipfx_ads_v9') ## CTSAF2 is my scratch space
data <- fetch(res,n = -1) 

```

```{r}
rm(list=setdiff(ls(), "data"))
gc()
df_medicare <- data
dim(df_medicare)
```

```{r}
#----------------
# Data Cleaning
#----------------

# Remove target information (247 -184 = 63)

drops <- c('mortality_365', 'readm_flag_365', 'er_365days', 're_claim_no',  're_clm_admsn_dt', 're_nch_bene_dschrg_dt','re_prvdr_num', 're_clm_utlztn_day_cnt', 're_clm_pmt_amt', 're_pdx', 're_ppx', 're_adm_source', 're_disc_status', 're_pdx_desc', 're_ppx_desc', 're_clm_admsn_dt_365', 're_nch_bene_dschrg_dt_365', 're_prvdr_num_365', 're_clm_utlztn_day_cnt_365', 're_clm_pmt_amt_365', 're_pdx_365', 're_ppx_365', 're_adm_source_365', 're_claim_no_365', 're_disc_status_365', 're_pdx_desc_365', 're_ppx_desc_365', 're_claim_no_365_up', 'readm_flag_365_up', 're_clm_admsn_dt_365_up', 're_nch_bene_dschrg_dt_365_up', 're_prvdr_num_365_up', 're_clm_utlztn_day_cnt_365_up', 're_clm_pmt_amt_365_up', 're_pdx_365_up', 're_ppx_365_up', 're_adm_source_365_up', 're_disc_status_365_up', 're_pdx_desc_365_up', 're_ppx_desc_365_up', 'er_claim_no_365', 'er_rev_cntr_dt_365', 'er_clm_thru_dt_365', 'er_prncpal_dgns_cd_365', 'er_pdx_desc_365', 'er_hcpcs_cd_365', 'er_hcpcs_desc_365', 'mortality_365_up', 'er_365days_up', 'er_claim_no_365_up', 'er_rev_cntr_dt_365_up', 'er_clm_thru_dt_365_up', 'er_prncpal_dgns_cd_365_up', 'er_pdx_desc_365_up', 'er_hcpcs_cd_365_up', 'er_hcpcs_desc_365_up','er_claim_no', 'er_rev_cntr_dt', 'er_clm_thru_dt','er_prncpal_dgns_cd','er_pdx_desc', 'er_hcpcs_cd', 'er_hcpcs_desc')


df_medicare <- df_medicare[, !(colnames(df_medicare) %in% drops)]


# Remove collinear features (184 - 149 = 33)
# also remove county region related information
drops <- c('cci_score_1825_days_b','elix_score_1825_days_b',  'fci_score_1825_days_b', 'follow_up_end_dt', 'follow_up_end_dt_365','ppx_desc', 'pdx_desc','hospital_name', 'fy', 'yr_adm', 'yr_disch', 'prvdr_state_name', 'prvdr_state_cd', 'prvdr_ssa_county_code', 'prov_vol_per_month', 'prvdr_home_hha_vol_month', 'prvdr_home_hha_vol_per', 'phy_vol_month', 'prvdr_urspa', 'prvdr_cbsa', 'prvdr_cbsa_desc', 'prvdr_msa','prvdr_msa_desc', "elix_cong_heart_fail_1825_days_b", "elix_periph_vas_dis_1825_days_b", "elix_paralysis_1825_days_b", "elix_copd_1825_days_b", "elix_aids_1825_days_b", "elix_met_cancer_1825_days_b", "fci_heart_attack_1825_days_b", "fci_obesity_1825_days_b", 'nch_bene_dschrg_dt','county', 'bene_cnty_cd',  'prvdr_division','prvdr_region','prvdr_region_cd', 'prvdr_div_code')

df_medicare <- df_medicare[, !(colnames(df_medicare) %in% drops)]


# Remove features same entry and unique entry (150 - 136 = 14)

drops <- c('version_id', 'cont_enroll_flag_1825b_89f', 'valid_date_of_death_1825b_89f', 'hmo_enroll_flag_1825b_89f', 'hmo_enroll_flag_365f','cont_enroll_flag_365f','at_physn_upin', 'op_physn_upin', 'ot_physn_upin',
           'desy_sort_key', 'claim_no', 'ot_physn_npi','at_physn_npi','op_physn_npi')


df_medicare <- df_medicare[, !(colnames(df_medicare) %in% drops)]



```

```{r}
#categorical regrouping

#disc_status

#Discharged to home/self-care
home = c("01", "21")
#Discharged to a hospital for care that wouldnâ€™t be covered under this episode
hospital_nc = c('04', '05', '63', '64', '65', '70')
#Left against medical advice or discontinued care. Patients who leave before triage or seen by physician
leave_early = '07'
#remove as 20 indicates the death of patients
remove = '20'
df_medicare$disc_status[!(df_medicare$disc_status %in% c(hospital_nc, home, leave_early, remove))] = 'post_acute_care'
df_medicare$disc_status[df_medicare$disc_status %in% home] = 'home'
df_medicare$disc_status[df_medicare$disc_status %in% hospital_nc] = 'hospital_nc'
df_medicare$disc_status[df_medicare$disc_status == leave_early] = 'leave_early'
df_medicare$disc_status[df_medicare$disc_status == remove] = 'remove'

#remove the row entries
df_medicare = df_medicare %>% filter(disc_status != 'remove')

#adm_source no regroup
#prncpal_dgns_cd: keep first digit
df_medicare$prncpal_dgns_cd = substr(df_medicare$prncpal_dgns_cd, 1, 1)

#icd_prcdr_cd1
hip = read.csv('icd_prcdr_cd1_hip', stringsAsFactors = FALSE)
hip = hip$icd_prcdr_cd1
df_medicare = df_medicare %>% filter(icd_prcdr_cd1 %in% hip)
df_medicare$icd_prcdr_cd1 = substr(df_medicare$icd_prcdr_cd1, 5, 5)
open = '0'
unkown = c('1', 'Z')
lap = c('3', '4', 'X')

df_medicare$icd_prcdr_cd1[df_medicare$icd_prcdr_cd1 == open] = 'open'
df_medicare$icd_prcdr_cd1[df_medicare$icd_prcdr_cd1 %in% unkown] = 'unkown'
df_medicare$icd_prcdr_cd1[df_medicare$icd_prcdr_cd1 %in% lap] = 'lap'
#clm_drg_cd no regroup
#gndr_cd
df_medicare$gndr_cd[df_medicare$gndr_cd == '1'] = 'M'
df_medicare$gndr_cd[df_medicare$gndr_cd == '2'] = 'F'
#bene_race
df_medicare$bene_race_cd[!(df_medicare$bene_race_cd %in% c('1','2'))] = 'Other'
df_medicare$bene_race_cd[df_medicare$bene_race_cd == '1'] = 'W'
df_medicare$bene_race_cd[df_medicare$bene_race_cd == '2'] ='B'
#drop disc_home_index as it duplicate with disc_status
df_medicare = df_medicare %>% select(-disc_home_index)
#provider_type no regroup needed
#hospital_name already drop
#prvdr_urgeo no regroup
#prvdr_region dropped due to too high level info
#prvdr_teaching_status no regroup

#bene_state_cd
bene_state_ab = read.csv('bene_state_cd',  colClasses=c("bene_state_cd"="character"),stringsAsFactors = FALSE )
df_join = left_join(df_medicare['bene_state_cd'], bene_state_ab, by = 'bene_state_cd')
df_medicare['bene_state_cd'] = df_join[, 3]
#bene_cnty_cd drop due to too low level detail
#prvdr_division drop due to too high level info
#prvdr_state_ab no regroup
#county drop due to too low level detail
write.csv(x = colnames(df_medicare), file = "colnames_population")
```


```{r}
# Remove missing value (426790 - 417395 = 9395)

# remove rows with missing value
df_medicare[df_medicare == -999] <- NA
df_medicare[df_medicare == 'NA'] <- NA
df_medicare <- df_medicare %>% drop_na(prvdr_num,prvdr_teaching_status,ma_pen_percent,prvdr_rday,ami_cabg)
```

```{r}
rm(list=setdiff(ls(), c("df_medicare", "model_all"))) #command to clean the environment list
gc()
```


```{r}
# The following code is to check whether there is still missing value left
t <- colSums(is.na(df_medicare))
t[t!=0]
```

```{r}

#-----------
# Hospital level and Population level Sampling
#-----------

##removing hospital with less than 100 observations and less than 67 positive or negative value for the target variables for validation set purpose
keep = df_medicare %>% 
  group_by(prvdr_num) %>% 
  summarise(total = n(), re_pos = sum(readm_flag), re_neg = total - re_pos, er_pos = sum(er_90days), er_neg = total - er_pos, mor_pos = sum(mortality_90), mor_neg = total - mor_pos) %>% filter(total > 99 & re_pos > 66 & re_neg > 66) %>% pull(prvdr_num)
#& er_pos > 10 & er_neg > 10 & mor_pos > 10 & mor_neg > 10 

#write.csv(keep, file = "keep_re_target66")

sample <- df_medicare %>% 
  filter(prvdr_num %in% keep)
```

```{r}
#one hot encoder
one_hot_name = c('disc_status', 'icd_prcdr_cd1', 'clm_drg_cd', 'gndr_cd', 'bene_race_cd', 'provider_type', 'prvdr_urgeo', 'prvdr_teaching_status')
df_one_hot = sample[, one_hot_name]
df_one_hot[] = lapply( df_one_hot, factor)
levels(df_one_hot$provider_type) = c(levels(df_one_hot$provider_type), 'Hospitals participating in ORD demonstration project') #put it as only one type of hospital left
one_hot_levels = lapply(df_one_hot, levels)
saveRDS(one_hot_levels, "one_hot_levels.Rds")

dmy <- dummyVars(" ~ .", data = df_one_hot)
dmy_transform <- data.frame(predict(dmy, newdata = df_one_hot))

sample = cbind(sample, dmy_transform)
#drop original one hot cat column and one of dummy group for dummy class = 2
drop1 = c('gndr_cd.M', 'provider_type.Hospitals.participating.in.ORD.demonstration.project','prvdr_teaching_status.No')
sample = sample[, !(colnames(sample) %in% c(one_hot_name, drop1))]


```

```{r}
# -------------------
# feature engineering
# -------------------

# Add time related features for later use and order data by date
sample$clm_admsn_dt = as.Date(sample$clm_admsn_dt)
sample$month = months(sample$clm_admsn_dt,abbreviate = TRUE)
sample$week_of_dates = weekdays(sample$clm_admsn_dt,abbreviate = TRUE)
sample = sample[order(sample$clm_admsn_dt),]

#change difftime object to numeric
sample$days_before_admsn = as.numeric(sample$clm_admsn_dt-as.Date('2016-01-01'), units="days")

#no need the days_before_dscharg as we already have length of stay and is removed from data in the collinearity section
#remove date column as we have convert it to number
drops = c('clm_admsn_dt')
sample = sample[,!(colnames(sample) %in% drops)]

#for later target encoder use
sample[sapply(sample, is.character)] <- lapply(sample[sapply(sample, is.character)], as.factor)
```


```{r}
# ----------------- 
# Data Preprocessing Function
# -----------------

#target encoding for categorical features

addrandom = function(x){
  return(x + runif(1))
}
targetencode = function(indices, df){
  result = c()
  df$fake_target = sapply(df$target, addrandom)
  for (i in indices){
    lookup = df %>% group_by_at(i) %>% summarise(mean(fake_target))
    result = c(result, lookup)
  }
  return(list(result, mean(df$fake_target)))
}

targetencode_transform = function(indices, result, df, dropna = TRUE, targetmean){
  j = 1
  for (i in indices){
    lookup = data.frame(result[j], result[j+1])
    cat_colname = colnames(df)[i]
    df_targetment = left_join(df[cat_colname], lookup, by = cat_colname)
    df[i] = df_targetment[, 2]
    j = j +2
  }
  if (dropna & sum(is.na(df))){
    print("There is missing categorical class")
    t <- colSums(is.na(df))
    print(t[t!=0])
    #df = df %>% drop_na()
    df[is.na(df)] = targetmean
  }
  return(df)
}
```

```{r}
# ----------------- 
# Train Model function
# -----------------
fitModel = function(df,type){
  
  folds <- 5
  cvIndex <- createFolds(df$target, folds, returnTrain = T)
  y_train = df$target
  st = Sys.time()
  if(type == 'rlg'){
    trcontrol = trainControl(index = cvIndex,
                              method = "cv",
                              number = folds,
                              summaryFunction = prSummary,
                              classProbs = TRUE)
    set.seed(0)
    fit <- train(
      form = target ~., 
      data = df,
      trControl= trcontrol,
      method = 'regLogistic',
      metric = "F")
  }
  else if(type == 'rf'){
    trcontrol = trainControl(index = cvIndex,
                              method = "cv",
                              number = folds,
                              summaryFunction = prSummary,
                              classProbs = TRUE)
    set.seed(0)
    fit <- train(
      form = target ~., 
      data = df,
      trControl= trcontrol,
      method = 'ranger',
      metric = "F")
  }
  else if(type == 'xgb'){
    df = df %>% select(-target)
    X_train = xgb.DMatrix(as.matrix(df))
    
    trcontrol = trainControl(
      index = cvIndex,
      method = "cv",
      number = folds,  
      allowParallel = TRUE,
      verboseIter = FALSE,
      classProbs = TRUE,
      summaryFunction = prSummary,
      returnData = FALSE
    )
    set.seed(0) 
    fit = train(
      X_train, y_train,  
      trControl = trcontrol,
      nthread = 1, #avoid double parallel computing causing the run time to expand 
      method = "xgbTree",
      metric = "F"
    )
  }
  ed = Sys.time()
  print(ed - st)
  return(fit)
}
```

```{r}
# ----------------- 
# F1 score function
# -----------------
#Note if TP + FP ==0 (when we all predict 0), precision is undefined.
#Precision = TP / (TP+FP)
#Recall = TP/(TP + FN)
#if TP + FN == 0 (when sample only have 0), recall is undefined
#F1 = 2 * Precision * Recall/ (Precision + Recall) -> undefined if TP = 0
# ConfusionMatrix
#         ACTUAL
#        1     0
# P    +----+----+
# R  1 | TP | FP |
# E    +----+----+
# D  0 | FN | TN |
#      +----+----+
# Since if we predict all as Class 0 or as Class 1, F1_score will return error, plus we don't want a model that predict all as 0 or 1, thus set its f1 score = -1 indicate bad model. If F1_Score returns an NaN, simply means TP = 0, thus we assigned f1_score = 0 in this case.
unbias_f1 = function(pred, yval){
  f1 = -1
  if (sum(pred)!= 0 & sum(pred)!= length(pred)){
    f1 = F1_Score( y_pred = pred, y_true = yval, positive = 1)
  }else if(sum(pred) == sum(yval)){
    f1 = 1.1
  }
  if(is.na(f1)){
    f1 = 0
  }
  return(f1)
}
# ----------------- 
# Prediction function
# -----------------
#scoring function returns a list of best Cross_Validation Score, Validation Set F1 scores, Log loss, AUC, and Accuracy with input model, X validation and y validation.
scoring = function(model, X_val, y_val){
  pre_class <- ifelse(predict(model, newdata = X_val, type = "raw") == "Class_0",0,1)
  pre_prob <- predict(model, newdata = X_val, type = "prob")$Class_1
  return(c(max(model$results$F, na.rm=TRUE), unbias_f1(pre_class, y_val), LogLoss(pre_prob, y_val), AUC(pre_prob, y_val), Accuracy(pre_class, y_val)))
}
```

```{r}
# ----------------- 
# train validation test split for hospital level and population model
# -----------------
# we focus on readmission now. so we remove mortality and er_90days 
drops<- c('mortality_90', 'er_90days')
sample <- sample[,!(colnames(sample) %in% drops)]
#rename the readm_flag as target here, so for other targets we can just change their name to target as well then able to use the following code
sample = sample %>% rename(target = readm_flag)
# create an empty list to store data
trains = list()
vals = list()
tests = list()

#start with five percent, if all then n_of_hos = keep
#n_of_hos = read.csv('prvdr_num_qualify_0.05',  colClasses=c("x"="character"))
n_of_hos = keep




#population target encoder  
tgen = readRDS(file = "pop_tgen100.Rds")
for(h in n_of_hos){
  temp = sample %>% filter(prvdr_num == h)
  temp <- temp[, !(colnames(temp) %in% c("prvdr_num"))]
  temp_0 = temp %>% filter(target == 0)
  temp_1 = temp %>% filter(target == 1)
  train_size_0 = as.integer(round(nrow(temp_0)*0.7, digits = 0))
  val_size_0 = as.integer(round(nrow(temp_0)*0.15, digits = 0))
  train_size_1 = as.integer(round(nrow(temp_1)*0.7, digits = 0))
  val_size_1 = as.integer(round(nrow(temp_1)*0.15, digits = 0))
    
  h_train = rbind(temp_0[c(1:train_size_0),],temp_1[c(1:train_size_1),])
  h_val = rbind(temp_0[c((train_size_0+1):(train_size_0 + val_size_0)),],
                temp_1[c((train_size_1+1):(train_size_1 + val_size_1)),])
  h_test = rbind(temp_0[c((train_size_0 + val_size_0+1):nrow(temp_0)),], 
                 temp_1[c((train_size_1 + val_size_1+1):nrow(temp_1)),])

  
  cat_name = names(Filter(is.factor, h_train))
  cat_index = match(cat_name, names(h_train))
  #use population encoder instead
  #tgen = targetencode(cat_index, h_train)
  
  train = targetencode_transform(cat_index, tgen[[1]], h_train, targetmean = tgen[[2]])
  val = targetencode_transform(cat_index, tgen[[1]], h_val, targetmean = tgen[[2]])
  test = targetencode_transform(cat_index, tgen[[1]], h_test, targetmean = tgen[[2]])
  
  #change target value to factor and assigned positive class to class1
  train$target =as.factor(train$target) 
  levels(train$target)= c("Class_0", "Class_1")
  train$target <- factor(train$target, levels=rev(levels(train$target)))
  
  #drops the hospital leaking information
  trains[[h]] = train
  vals[[h]] = val
  tests[[h]] = test
}
```


```{r}
# a*p_model + (1-a)*h_model
ensemble = function(p_model, h_model, h_model_var, df, ptype, htype, as){
  #as = c(0.0, 1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
  y_val = df$target
  if(is.factor(y_val)){
    y_val = (as.numeric(y_val) - 2) * -1 #change level to numeric Class0 = 2 Class1 = 1 as we do as.numeric
  }
  p_Xval = df[, p_model$coefnames]
  h_Xval = df[, h_model_var]
  if(ptype == "xgb"){
     p_Xval = xgb.DMatrix(as.matrix(p_Xval))
  }
  if(htype == "xgb"){
    h_Xval = xgb.DMatrix(as.matrix(h_Xval))
  }
  p_prob <- predict(p_model, newdata = p_Xval, type = "prob")$Class_1
  h_prob <- predict(h_model, newdata = h_Xval, type = "prob")$Class_1
  en_f1 = c()
  lglosses =c()
  aucs = c()
  accs = c()
  for(a in as){
    ph_prob = a*p_prob + (1-a)*h_prob
    ph_pred = floor(ph_prob + 0.5) #use floor to avoid round(0.5) = 0 issue
    en_f1 = c(en_f1, unbias_f1(ph_pred, y_val))
    lglosses = c(lglosses, LogLoss(ph_prob, y_val))
    aucs = c(aucs, AUC(ph_prob, y_val))
    accs = c(accs, Accuracy(ph_pred, y_val))
  }
  #return(en_f1)
  return(data.frame("alpha" =as, "en_f1" = en_f1, "en_lgloss" = lglosses, "en_auc" = aucs, "en_acc" = accs))
}
```



```{r}
# This is the function that takes in the train set with the desire sample size with down/up/not sampling
# the class1 of the target. Then split the train variables into  base, Boruta, Boruta + time, base + time combination.
# Finally, train each combination with regularize logistic regression, random forest, and XGBoost model, 
# and evaluate the F1 score base on input validation set.
# The output is a list contain result summary data frame and a list of the fitted models

#last year top 10, only 9 due to time of discharge is remove due to collinearity

hospital_model = function(train, val, imbalance = "NA", population_model){
  #alpha to try for ensemble method
  as = c(0.0, 1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
  #population models
  
  original_train = nrow(train)
  pop_val = val
  ptype ="rf"
  pop_yval = pop_val$target
  pop_Xval = pop_val[, population_model$coefnames]
  pop_sc = scoring(population_model, pop_Xval, pop_yval)
  
  # Remove columns leaking hospital information
  drops <- c('prov_vol_annual', 'prov_vol_per_month', 'prvdr_home_hha_vol', 'prvdr_home_hha_vol_month', 'prvdr_home_hha_vol_per', 'phy_vol_annual', 'phy_vol_month', 'op_phy_cum_exp', 'provider_type', 'hospital_name', 'prvdr_ssa_county_code', 'prvdr_urgeo', 'prvdr_urspa', 'prvdr_wi', 'prvdr_cola', 'prvdr_resident_to_bed_ratio', 'prvdr_rday', 'prvdr_beds', 'prvdr_dshpct', 'prvdr_dshopg', 'prvdr_dshcpg', 'prvdr_state_ab', 'bpci_lejr', 'cjr', 'bpci_ami_cabg', 'ami_cabg', 'prvdr_cbsa', 'prvdr_cbsa_desc', 'prvdr_msa', 'prvdr_msa_desc', 'prvdr_teaching_status', 'low_income_subsidy', 'poverty_per', 'grp_1', 'grp_2', 'grp_3', 'grp_4', 'grp_5', 'grp_6', 'grp_7', 'grp_8', 'grp_9', 'grp_10', 'unemp_percentage', 'no_of_snfs', 'ma_pen_percent', 'snf_per_capita')
  
  val = val[,!(colnames(val) %in% drops)]
  train = train[,!(colnames(train) %in% drops)] 
  
  #imbalance data set treatment
  if(imbalance == "down"){
    set.seed(9560)
    train <- downSample(x = train[, !(colnames(train) %in% c("target"))], y = train$target)
    train = train %>% rename(target = Class)
  }else if(imbalance == "up"){
    train <- upSample(x = train[, !(colnames(train) %in% c("target"))], y = train$target)
    train = train %>% rename(target = Class)
  }
  
  #Boruta, Boruta + time as index 1, 2
  feature_set = list()
  
  ## Boruta with no time data set
  boruta_start_time <- Sys.time()
  sigtest <- Boruta(target~. , data = train[!(colnames(train) %in% c("days_before_admsn", "month", "week_of_dates"))], holdHistory=FALSE, doTrace=2)
  boruta_end_time <- Sys.time()
  boruta_features = c(getSelectedAttributes(sigtest))
  feature_set[[1]] = boruta_features
  
  print(paste("Number of boruta features selected:", as.character(length(boruta_features)), sep = " "))
  print(boruta_end_time - boruta_start_time)
  
  ## Boruta with time data set
  time_feature = c('month','week_of_dates','days_before_admsn')
  boruta_time_features = c(boruta_features, time_feature)
  feature_set[[2]] = boruta_time_features
  
  ## top features selected from last year
  top_10_2019 = c('prior_inp_only_los', 'total_prior_los', 'days_before_admsn', 'prior_hha_visits', 'clm_utlztn_day_cnt',  'index_los', 'clm_pps_cptl_drg_wt_num', 'prior_out_visits', 'prior_snf_los')
  feature_set[[3]] = top_10_2019
  
  ## select k best approach
  k_start_time <- Sys.time()
  n_k = as.integer(round(nrow(train)/20, digits = 0))
  evaluator <- filterEvaluator('chiSquared')
  directSearcher = directSearchAlgorithm('selectKBest', list(k=n_k))
  k_best_results = directFeatureSelection(train, 'target', directSearcher, evaluator)
  k_best_features = c(k_best_results$featuresSelected)
  feature_set[[4]] = k_best_features
  print(paste("Number of kbest features selected:", as.character(length(k_best_features)), sep = " "))
  print(Sys.time() - k_start_time)
  
  #training models
  model_types = c("rlg", "rf", "xgb") #Mike run this version
  #model_types = c("rf", "xgb", "rlg") #Qiang run this version
  #model_types = c("xgb", "rlg", "rf") #Zining run this version
  X_types = c("Boruta", "Boruta_time", "2019_top10", "Kbest")
  model_X_types = c()
  cv_f1s = c()
  val_f1s = c()
  loglosses = c()
  AUCs = c()
  accuracys = c()
  en_a = c()
  en_valf1 = c()
  en_loglosses = c()
  en_AUCs = c()
  en_accuracys = c()

  # models = list()
  st = Sys.time()
  rt= c()
  rt_unit = c()
  population_vf1s = c()
  best_hvalf1 = -2
  h_tiebreaker_lg = 1000
  best_envalf1 = -2
  en_tiebreaker_lg = 1000
  all_models = list()
  j = 1
  for(type in model_types){
    for(i in c(1:4)){
      model_X_type = paste(type, X_types[i], sep= "_")
      print(model_X_type)
      model_X_types = c(model_X_types, model_X_type)
      
      train_df = train[,c(feature_set[[i]], "target")]
      y_val = val$target
      X_val = val[,feature_set[[i]]]
      model_var = colnames(X_val)
      if(type == "xgb"){
        X_val = xgb.DMatrix(as.matrix(X_val))
      }
      st_i= Sys.time()
      #train the hospital level model
      model = fitModel(train_df, type)
      all_models[[j]] = model
      j = j+1
      t = Sys.time()-st_i
      #record the scoring result
      rt_unit = c(rt_unit, units(t))
      rt = c(rt, t)
      sc = scoring(model, X_val, y_val)
      cv_f1s = c(cv_f1s, sc[1])
      val_f1s = c(val_f1s, sc[2])
      loglosses = c(loglosses, sc[3])
      AUCs = c(AUCs, sc[4])
      accuracys = c(accuracys, sc[5])
      
      #training ensemble method on training set to get best alpha 
      # a*p_model + (1-a)*h_model
      en_val_result = ensemble(population_model, model, model_var, pop_val, ptype, type, as)
      max_index = which.max(en_val_result$en_f1)
      # list(max(en_f1), as[which.max(en_f1)] 
      #      (data.frame("alpha" =as, "en_f1" = en_f1, "en_lgloss" = lglosses, "en_auc" = aucs, "en_acc" = accs))
      a = en_val_result$alpha[max_index]
      en_a = c(en_a, a)
      en_f1_temp = en_val_result$en_f1[max_index]
      en_valf1 = c(en_valf1, en_f1_temp)
      en_loglosses = c(en_loglosses, en_val_result$en_lgloss[max_index])
      en_AUCs = c(en_AUCs, en_val_result$en_auc[max_index])
      en_accuracys = c(en_accuracys, en_val_result$en_acc[max_index])
      
      #select best hospital model base on hospital vf1 score
      if(sc[2] > best_hvalf1){
          best_hvalf1 = sc[2]
          best_model = model
          h_tiebreaker_lg = sc[3]
          best_hos_model_type = model_X_type
      }else if(sc[2] == best_hvalf1){
        if(sc[3] < h_tiebreaker_lg){
          best_hvalf1 = sc[2]
          best_model = model
          h_tiebreaker_lg = sc[3]
          best_hos_model_type = model_X_type
        }
      }
      
      #select best hospital model base on ensemble valf1 score
      if(en_f1_temp > best_envalf1){
          best_envalf1 = en_f1_temp
          best_enmodel = model
          best_alpha = a
          en_tiebreaker_lg = en_val_result$en_lgloss[max_index]
          best_en_model_type = model_X_type
      }else if(en_f1_temp == best_envalf1){        
        if(en_val_result$en_lgloss[max_index] < en_tiebreaker_lg){
          best_envalf1 = en_f1_temp
          best_enmodel = model
          best_alpha = a
          en_tiebreaker_lg = en_val_result$en_lgloss[max_index]
          best_en_model_type = model_X_type
        }
      }
    }
  }
  print(Sys.time()-st)

  #Summary_result
  summary_result <- data.frame("Type" =model_X_types, "cv_f1" = cv_f1s, "val_f1" = val_f1s, 
                               "logloss" = loglosses,"AUC" = AUCs, "accuracy" = accuracys,
                               "runtime" = rt, "time_unit" = rt_unit, "alpha" = en_a,
                               "ensemble_valf1" = en_valf1, "en_logloss" = en_loglosses,
                               "en_AUC" = en_AUCs, "en_accuracy" = en_accuracys,stringsAsFactors = FALSE)

  summary_result$pop_cv_f1 = pop_sc[1]
  summary_result$pop_val_f1 = pop_sc[2]
  summary_result$pop_logloss = pop_sc[3]
  summary_result$pop_AUC = pop_sc[4]
  summary_result$pop_accuracy = pop_sc[5]
  summary_result$train_osize = original_train
  summary_result$train_upsize = nrow(train)
  summary_result$train_1 = as.vector(table(train$target))[1]
  summary_result$val_size = nrow(val)
  summary_result$val_1 = as.vector(table(val$target))[2]
  summary_result$n_boruta = length(boruta_features)
  summary_result$n_kbest = length(k_best_features)
  
  summary_result[is.na(summary_result)] <- 0
  best_cvf1 = max(summary_result$val_f1)
  best_index = which.max(summary_result$val_f1)
  best_model_name = model_X_types[best_index]
    
  print(paste("The best model is:", best_model_name, "with validation f1 score:", best_cvf1, sep = " "))
  result = list("result_df" = summary_result, "best_hos_model" = best_model, "boruta" = boruta_features, "k_best" = k_best_features, "best_hos_model_en" = best_enmodel, "alpha" = best_alpha, "best_hos_model_type" = best_hos_model_type, "best_en_model_type" = best_en_model_type, "all_models"=all_models)
  return(result)
}
```

```{r message=FALSE}
#testing for single hospital
# train = trains[["010001"]]
# val = vals[["010001"]]
# result = hospital_model(train, val, imbalance = "up", p_model)
```
```{r} 
# read in previosuly trained population model
p_model   = readRDS(file = "rf_basetime_100_4.Rds")
```
```{r}
#save the train val test split data to keep for next time use
saveRDS(trains, "trains_RA367")
saveRDS(vals, "vals_RA367")
saveRDS(tests, "tests_RA367")
```

```{r message=FALSE}
#rm(list=setdiff(ls(), c("trains", "vals", "tests", "p_model"))) command to clean the environment list
#for loop to capture all hospital
st = Sys.time()
hos_results = list()

n_of_hos = read.csv("keep_re_target66", colClasses=c("x"="character"))
n_of_hos = n_of_hos$x

#spliting work for each hospital
#Mike use this version
i = 1 #Mike
e = 367 #Mike

#Qiang use this version
# i = 1
# e = 120

#Zining use this version
# i = 241
# e = 367

# Write each hospital's hospital model and ensemble model into one RDS file
for(h in n_of_hos[i:e]){
  rds = list()
  train = trains[[h]]
  val = vals[[h]]
  result = hospital_model(train, val, imbalance = "up", p_model)
  rds[[h]] = result
  #hos_results[[h]] = result
  filename =paste("hos_results_newpop/hos_results_new_", i, ".Rds", sep = "")
  saveRDS(rds, filename)
  i = i+1
}
ed = Sys.time()
print(ed - st)
```


```{r}
# 
# #create data frame for plotting
# 
# h_ns_best_hmodel = hos_results[[1]]$result_df[0,]
# h_ns_best_enmodel = hos_results[[1]]$result_df[0,]
# for(h_result in hos_results){
#   h_result_df = h_result$result_df
#   best_h_indexes = which(h_result_df$val_f1 %in% max(h_result_df$val_f1))
#   if(length(best_h_indexes > 1)){
#     tiebreaker = h_result$result_df[best_h_indexes, ]
#     best_h_index = which.min(tiebreaker$logloss)
#     h_ns_best_hmodel = rbind(h_ns_best_hmodel, tiebreaker[best_h_index,])
#   }else{
#     best_h_index = best_h_indexes[1]
#     h_ns_best_hmodel = rbind(h_ns_best_hmodel, h_result_df[best_h_index,])
#   }
#   best_en_indexes = which(h_result_df$ensemble_valf1 %in% max(h_result_df$ensemble_valf1))
#   if(length(best_en_indexes > 1)){
#     tiebreaker = h_result$result_df[best_en_indexes, ]
#     best_en_index = which.min(tiebreaker$en_logloss)
#     h_ns_best_enmodel = rbind(h_ns_best_enmodel, tiebreaker[best_en_index,])
#   }else{
#     best_en_index = best_en_indexes[1]
#     h_ns_best_enmodel = rbind(h_ns_best_enmodel, h_result_df[best_en_index,])
#   }
# }
# 
# #result based on max hospital validation f1 score
# h_ns_best_hmodel = cbind(half_n_of_hos, h_ns_best_hmodel)
# #result based on max ensemble validation f1 score
# h_ns_best_enmodel = cbind(half_n_of_hos, h_ns_best_enmodel)
# write.csv(x = h_ns_best_hmodel, file = "h_ns_best_hmodel_121_140_new")
# write.csv(x = h_ns_best_enmodel, file = "h_ns_best_enmodel_121_140_new")
```


